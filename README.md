Goal: understand transformers so deeply that we can transcend it and invent the next architecture.

Put "Coded & trained LLM like Anthropic's Claude 4 from the group up" on your resume.

## Matrix multipliecation
- UI tools and tasks, game

## FP8 vs FP16 vs BF16 vs FP4
- [GitHub](https://github.com/vukrosic/nn-precision-comparison)

- Learn To Convert Numeric Data Types
    - 2x / 4x / 8x faster AI training and same memory reduction
    - [Google Colab](https://colab.research.google.com/drive/1NOC3wLWLxNUY-_rki0SXzpov26ajFSW0?usp=sharing)
    - [ChatGPT](https://chatgpt.com/share/68972df4-d218-8002-a734-a4793430927a)

### FP4

YouTube - https://youtu.be/UqN0cNfKbAM

Bilibili - http://bilibili.com/video/BV1yybKzmEvF/

GitHub - https://github.com/vukrosic/llm-fp4-pretraining-research

Colab - https://colab.research.google.com/drive/1c6bC6x_9-99qNmmC5zm0v2GOasZSTzun?usp=sharing

Code LLM (GPT-5, DeepSeek, Qwen3, Llama4) from scratch - https://github.com/vukrosic/open-source-ai-2026



## Single neuron forward pass

## Derivatives & Gradients
- A little bit of theory
    - How to think of derivatives intuitively, what do they do (especially in nenuraln networks)
- Excercises (Google Colab, Kaggle, GitHub, Jupyther)
- Chain Rule
- Jacobian & Hessian
- Numerical Gradients
- Autograd



## Backpropagation

## SGD

## Actication Functions

## MLP

## Attention Mechanism
- [Attention Mechanism Theory - Start at 1:32:43](https://youtu.be/wcDV3l4CD14?t=5563)
- Fun Attention Experiment To Pay Attention To The Biggest Number (SHOCKING Results)
    - [YouTube](https://youtu.be/xsgaI-udBWc)
    - [Google Colab](https://colab.research.google.com/drive/1fXBwHN5tWkE_Kh8jVmfUoyNLrLum_eL6?usp=sharing)
    - [Bilibili](https://www.bilibili.com/video/BV1z9tazCEEf/)
- GQA
- MLA

## RoPE

## Attention & RoPE Interaction
- 

## Feedforward Network In Transformer
- SwiGLU

## AdamW

## Muon

## Softmax

## RMSNorm



## Distributed Training

## Cuda basics (no need for more for now)
- Google Colab, Kaggle, GitHub, iplynb