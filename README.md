Goal: understand transformers so deeply that we can transcend it and invent the next architecture.

## Matrix multipliecation
- UI tools and tasks, game

## FP8 vs FP16 vs BF16 vs FP4
- [GitHub](https://github.com/vukrosic/nn-precision-comparison)

- Learn To Convert Numeric Data Types
    - 2x / 4x / 8x faster AI training and same memory reduction
[Google Colab](https://chatgpt.com/share/68972df4-d218-8002-a734-a4793430927a)

## Single neuron forward pass

## Derivatives & Gradients
    - A little bit of theory
        - How to think of derivatives intuitively, what do they do (especially in nenuraln networks)
    - Excercises (Google Colab, Kaggle, GitHub, Jupyther)
    - Chain Rule
    - Jacobian & Hessian
    - Numerical Gradients
    - Autograd



## Backpropagation

## SGD

## Actication Functions

## MLP

## Attention Mechanism
- GQA
- MLA

## RoPE

## Attention & RoPE Interaction

## Feedforward Network In Transformer
- SwiGLU

## AdamW

## Muon

## Softmax

## RMSNorm



## Distributed Training

## Cuda basics (no need for more for now)
- Google Colab, Kaggle, GitHub, iplynb